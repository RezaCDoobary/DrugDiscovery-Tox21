{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oAWpYdVE1C2"
   },
   "source": [
    "# Deep neural network modelling\n",
    "\n",
    "In this notebook, a deep nueral network is applied to the drug discovery exericse of the Tox-21 challenge. We divide up the exercise in 6 main components.\n",
    "\n",
    "1. Imports and code admin (particular since this notebook has been run on google colab for the accesiility of the GPU.)\n",
    "\n",
    "2. Obtaining the data from the previous preprocessings. This is supplmented with a few basic data continuity checks.\n",
    "\n",
    "3. Splitting the data into training and testing sets for each data type input.\n",
    "\n",
    "5. Data engineering, for example, the bag of words paradigm for the smiles but also some PCA is introduce for the sparse datasets.\n",
    "\n",
    "6. A grid search over specific data inputs and DNN layers is added.\n",
    "\n",
    "For the google colab - the basic presciption is to copy the drugdiscovery library into a google drive folder and import the library in the colab notebook from the drive. Accessing google drive is done in the following step below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "wRsGx2-duWKO",
    "outputId": "fa15e9a2-f714-4d3d-e68f-aef91538f16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "root = '/content/gdrive/My Drive/drug_discovery/DrugDiscovery/'\n",
    "# root is the root folder where the library is held."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "504suQK9GWQz"
   },
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Oovy4t8sunes",
    "outputId": "316c7e81-03d0-42a8-cc8d-cd6144f9956e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue if using colab and not planning to use rdkit\n",
      "Continue if using colab and not planning to use rdkit\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Sklearn data preprocessing imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# drugdiscovery code imports\n",
    "import sys\n",
    "sys.path.insert(1, root)\n",
    "sys.path.insert(1, root + 'drugdiscovery')\n",
    "\n",
    "import drugdiscovery as dd\n",
    "from drugdiscovery import preprocessing as pp\n",
    "from drugdiscovery import deeplearning as dl\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHDpJiiUH5d3"
   },
   "source": [
    "2. ### Obtaining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOhALiTZgpv2"
   },
   "outputs": [],
   "source": [
    "source_data = root+'data/data_dups_removed_with_H.csv'\n",
    "molecular_descriptors = root + 'data/molecular_descriptors_from_source.csv'\n",
    "fingerprint = root + 'data/morgan_fingerprint_from_source_10.csv'\n",
    "toxicophores = root +'data/known_toxic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P36A8XHQwWiQ"
   },
   "outputs": [],
   "source": [
    "source_data, molecular_descriptors, fingerprint, toxicophores = \\\n",
    "[pd.read_csv(x, index_col = 0) for x in [source_data, molecular_descriptors, fingerprint, toxicophores]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2rJStRA9KSQM",
    "outputId": "62a03f2e-7c1f-472e-f993-9978497f7bd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, False]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.isnull().sum().any() for x in [source_data, molecular_descriptors, fingerprint, toxicophores]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEjurymDIeXt"
   },
   "source": [
    "So it would seem that there are some nulls for the source data, however this comes from the face that not all targets are filled - this will taken care of with masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "F_OO2IglIF38",
    "outputId": "f65a10c6-8186-42fe-bd13-0e52e28777bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SR-HSE           1425\n",
       "NR-AR             575\n",
       "SR-ARE           2082\n",
       "NR-Aromatase     2077\n",
       "NR-ER-LBD         905\n",
       "NR-AhR           1327\n",
       "SR-MMP           2101\n",
       "NR-ER            1708\n",
       "NR-PPAR-gamma    1436\n",
       "SR-p53           1112\n",
       "SR-ATAD5          787\n",
       "NR-AR-LBD        1116\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data.isnull().sum()[source_data.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rsH_qszt0RMs",
    "outputId": "46a163ad-3da5-417a-d96f-01f1da1ba022"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test that all dfs that the same indices since they have non-trivial indices due to duplicate deletion.\n",
    "[(source_data.index == x.index).all() for x in [molecular_descriptors, fingerprint, toxicophores]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klTHhwg6PtRG"
   },
   "source": [
    "### 3. Separating into training and testing\n",
    "\n",
    "In this section we will do the three tasks:\n",
    "\n",
    "1. We will consider the target data set from the source data. We will need to create masks for the datapoints where there are null values. This is so that any training or metric computation ignores this in the target.\n",
    "\n",
    "2. We will then separate the data and the masks into training/training data.\n",
    "\n",
    "3. Use the training/testing index structure to deduce the same training/testing for the other data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUKss1N5RBFw"
   },
   "source": [
    "We start by extracting the raw data from the source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tg-3JC70RGDS"
   },
   "outputs": [],
   "source": [
    "source_features = ['FW','SMILES']\n",
    "targets = ['SR-HSE','NR-AR', 'SR-ARE', 'NR-Aromatase', 'NR-ER-LBD', 'NR-AhR', 'SR-MMP',\\\n",
    "       'NR-ER', 'NR-PPAR-gamma', 'SR-p53', 'SR-ATAD5', 'NR-AR-LBD']\n",
    "\n",
    "\n",
    "#dealing the source data\n",
    "raw_y = source_data[targets]\n",
    "raw_X = source_data[source_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxEOh6x7RNny"
   },
   "source": [
    "Next, we know that `raw_y` will have null from which we would like to create masks from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tblQezV4RNXf"
   },
   "outputs": [],
   "source": [
    "null_mask = np.array(np.logical_not(raw_y.isnull().values),int)\n",
    "raw_y = raw_y.fillna(0.0)\n",
    "mask_df = pd.DataFrame(null_mask, columns = [r+'_mask' for r in raw_y.columns], index = raw_y.index)\n",
    "\n",
    "# The masks are attached to the raw target set, so it is easier to move this data around.\n",
    "raw_y = pd.concat([raw_y,mask_df],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8qnwfkhRhFu"
   },
   "source": [
    "Next, the data is split into training/testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMCBlAVsRf_F"
   },
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "X_train_source, X_test_source, y_train, y_test = train_test_split(raw_X, raw_y, test_size = test_size, random_state=42)\n",
    "\n",
    "training_index = X_train_source.index\n",
    "testing_index = X_test_source.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMbYIbFHRnYu"
   },
   "source": [
    "The indeices `training_index` and `testing_index` are non-trvial and are kept to be applied to the ther data inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMQMQquA0USD"
   },
   "outputs": [],
   "source": [
    "#fingerprints\n",
    "fp_cols = list(fingerprint.columns)\n",
    "fp_cols.remove('DSSTox_CID')\n",
    "fp_cols.remove('SMILES')\n",
    "X_train_fp, X_test_fp = fingerprint[fp_cols].loc[training_index],fingerprint[fp_cols].loc[testing_index]\n",
    "\n",
    "# descriptors\n",
    "desc_cols = list(molecular_descriptors.columns)\n",
    "desc_cols.remove('SMILES')\n",
    "X_train_desc, X_test_desc = molecular_descriptors[desc_cols].loc[training_index],\\\n",
    "                                                molecular_descriptors[desc_cols].loc[testing_index]\n",
    "\n",
    "# known toxic\n",
    "X_train_tox, X_test_tox = toxicophores.loc[training_index],\\\n",
    "                                                toxicophores.loc[testing_index]\n",
    "\n",
    "# finally, we separate out the training from the mask data.\n",
    "y_train, mask_train = y_train[targets],y_train[mask_df.columns]\n",
    "y_test, mask_test = y_test[targets],y_test[mask_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uvBXN45SUPg"
   },
   "source": [
    "So we now have the four data sources:\n",
    "\n",
    "1. Source data (which includes the smiles and molecular weight)\n",
    "2. Fingerprints\n",
    "3. The molecular descriptors of each molecule\n",
    "4. The dice similarity for the fingerprints of molecular scaffolds with known toxicophores.\n",
    "5. We also have the training/testing masks which we will use to mask over the nulls in the target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymb4ALDz0jEP"
   },
   "source": [
    "# 4. Data engineering\n",
    "\n",
    "In this section we perform some data engineering tasks:\n",
    "1. Extract the molecular bag of words\n",
    "2. Standardise all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G90MC4gBwiUa"
   },
   "outputs": [],
   "source": [
    "def transform(train, test, apply_transformer):\n",
    "  train_new = apply_transformer.fit_transform(train)\n",
    "  test_new = apply_transformer.transform(test)\n",
    "  return train_new, test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSCF-Y3W0ljF"
   },
   "outputs": [],
   "source": [
    "# The source dataset\n",
    "smiles = X_train_source['SMILES'].values\n",
    "bow = pp.BagOfWordsMols(smiles)\n",
    "bow_train = bow.fit()\n",
    "bow_test = bow.transform(X_test_source['SMILES'].values)\n",
    "\n",
    "bow_train = np.insert(bow_train, 0, X_train_source['FW'], 1)\n",
    "bow_test = np.insert(bow_test, 0, X_test_source['FW'], 1)\n",
    "\n",
    "# Standardise the data\n",
    "bow_train, bow_test = transform(bow_train, bow_test, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQcNJtlyPvBj"
   },
   "outputs": [],
   "source": [
    "# Standardise the toxicophores dataset\n",
    "X_train_tox, X_test_tox = transform(X_train_tox, X_test_tox, StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKpxeXXIsCSp"
   },
   "outputs": [],
   "source": [
    "#standardise the molecular description dataset\n",
    "X_train_desc, X_test_desc = transform(X_train_desc, X_test_desc, StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FksBxiKrpcUx"
   },
   "source": [
    "So, at this juncture - our total data sourcing is as follows:\n",
    "\n",
    "1. bow_train, bow_test : standardised molecular bag of words of the source smiles.\n",
    "\n",
    "2. X_train_fp, X_test_fp : The molecular fingerprint of the candidate molecules.\n",
    "\n",
    "3. X_train_tox, X_test_tox : standardised dice similarities between kwwn toxicophores and scaffolds of the candidate molecules.\n",
    "\n",
    "4. X_train_desc, X_test_desc : standardised molecular descriptors for the candidate molecules.\n",
    "\n",
    "5. y_train, y_test : target variable that we want , nulls included.\n",
    "\n",
    "6. mask_train, mask_test : mask dataset which masks over the nulls in y_train and y_test, this is so that the nulls targets are not included in back propagation in the neural networks and they are not used in any metric computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BeWxwwj00B_"
   },
   "source": [
    "# 5. Modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_Yq8OVDrbsc"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_train_fp_svd, X_test_fp_svd = transform(X_train_fp, X_test_fp, TruncatedSVD(1024))\n",
    "bow_train_svd, bow_test_svd = transform(bow_train, bow_test, TruncatedSVD(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FYA1-4sFMmQ"
   },
   "source": [
    "We now have two mode datasets that we can add to the data input space - but we shall take X_train_fp OR X_train_fp_svd and bow_train OR bow_train_svd, as well as similary for the test side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irV7P8yv0Ib8"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def prepare_data(with_pca, batch_size, y_train, y_test, mask_train, mask_test, training_data, testing_data):\n",
    "  \"\"\"\n",
    "  Always use PCA to half the size.\n",
    "  \"\"\"\n",
    "  X_train = np.concatenate(training_data,1)\n",
    "  X_test = np.concatenate(testing_data,1)\n",
    "\n",
    "  if with_pca:\n",
    "    N,p = X_train.shape\n",
    "    pca_shape = int(p/2)\n",
    "    pca = PCA(pca_shape)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "  train_set, test_set, train_loader, number_of_batches = dl.get_data(X_train, y_train, mask_train, X_test, y_test, mask_test,batch_size)\n",
    "  return train_set, test_set, train_loader, number_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdAkNReHG4ya"
   },
   "outputs": [],
   "source": [
    "# we enumerate valid configurations here (we write them out as strings to save on local space)\n",
    "\n",
    "train_inputs = [\n",
    "          #'bow_train',\n",
    "          'X_train_fp',\n",
    "          'X_train_tox',\n",
    "          'X_train_desc',\n",
    "]\n",
    "import collections\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK0iOU-uHXRE"
   },
   "outputs": [],
   "source": [
    "all_combinations = []\n",
    "for r in range(len(train_inputs) +1):\n",
    "\n",
    "    combinations_object = itertools.combinations(train_inputs, r)\n",
    "    combinations_list = list(combinations_object)\n",
    "    all_combinations += combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJE9lLxnI-j2"
   },
   "outputs": [],
   "source": [
    "training_configurations = []\n",
    "for x in all_combinations[1:]:\n",
    "  config = '[' + ','.join(list(x)) + ']'\n",
    "  training_configurations.append(config)\n",
    "  if 'bow_train' in config:\n",
    "    config = config.replace(\"bow_train\", \"bow_train_svd\")\n",
    "    training_configurations.append(config)\n",
    "  if 'X_train_fp' in config:\n",
    "    config = config.replace(\"X_train_fp\", \"X_train_fp_svd\")\n",
    "    training_configurations.append(config)\n",
    "\n",
    "testing_configurations = []\n",
    "for configs in training_configurations:\n",
    "    config = configs.replace(\"train\", \"test\")\n",
    "    testing_configurations.append(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7Ybz6dpXMDHH",
    "outputId": "cb65f835-9bd2-4a1b-a0ec-d0b13c6b45d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_configurations) == len(testing_configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Q2RInLtEmrZB",
    "outputId": "fb531d28-102f-4c54-e33b-04d794e1f0f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[X_train_fp]',\n",
       " '[X_train_fp_svd]',\n",
       " '[X_train_tox]',\n",
       " '[X_train_desc]',\n",
       " '[X_train_fp,X_train_tox]',\n",
       " '[X_train_fp_svd,X_train_tox]',\n",
       " '[X_train_fp,X_train_desc]',\n",
       " '[X_train_fp_svd,X_train_desc]',\n",
       " '[X_train_tox,X_train_desc]',\n",
       " '[X_train_fp,X_train_tox,X_train_desc]',\n",
       " '[X_train_fp_svd,X_train_tox,X_train_desc]']"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FgQSpQ_QmwKN",
    "outputId": "7a022b89-60b0-42cb-ace9-687b738d0cc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2RtviF6duU_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def run(epochs, layers, config_n, final_pca):\n",
    "  print('Working in config ',config_n)\n",
    "  print('INPUTS : ',training_configurations[config_n])\n",
    "  print('LAYERS : ',layers)\n",
    "  print('WITH FINAL PCA : ',final_pca)\n",
    "\n",
    "  train_set, test_set, train_loader, number_of_batches = prepare_data(final_pca, 128,  y_train, y_test, mask_train, \\\n",
    "                                                                        mask_test, eval(training_configurations[config_n]),eval(testing_configurations[config_n]))\n",
    "  p = train_set[0][0].shape[0]\n",
    "  activations = [torch.relu]*len(layers)\n",
    "  early_stopper = dl.EarlyStopping(patience=10)\n",
    "  model = dl.net(p, 12, seed = 12345, hidden_layers = layers, activations = activations).to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=4e-5,weight_decay= 1e-5)\n",
    "  criterion = nn.BCELoss()\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose = False)\n",
    "  training_module = dl.Trainer(model, optimizer,criterion,epochs, root, device, scheduler ,early_stopper)\n",
    "  model_name = 'config_N'+ str(config_n) +'_'+'_'.join([str(l) for l in layers])\n",
    "  results = training_module.train_model(train_loader, test_set, number_of_batches, targets, mask_train, mask_test, model_name)\n",
    "  if final_pca:\n",
    "    results.to_csv(root + 'models/dnn_models/' + model_name + '_wpca.csv')\n",
    "  else:\n",
    "    results.to_csv(root + 'models/dnn_models/' + model_name + '.csv')\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi6N7e9j0-pJ"
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "layers = [[1024],[1024,2048],[1024,2048,4196]]\n",
    "\n",
    "\n",
    "for k in [i for i in range(10,11)]:  \n",
    "  for layer_config in layers:\n",
    "    for with_pca in [True, False]: \n",
    "      run(epochs, layer_config,  k, with_pca)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "drugdiscdnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
